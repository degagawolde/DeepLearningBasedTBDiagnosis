{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import all packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m glob\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, Activation, BatchNormalization,MaxPooling2D,Dense,concatenate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization,MaxPooling2D,Dense,concatenate\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate,Conv2DTranspose\n",
    "from tensorflow.keras.layers import Input,UpSampling3D,Dense, Dropout,BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,LearningRateScheduler\n",
    "from tensorflow.keras.metrics import Recall, Precision,MeanIoU,AUC\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "EPOCHS = 50\n",
    "BATCH = 4\n",
    "LR = 1e-4\n",
    "\n",
    "PATH = './dataset/newdataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path='../dataset/newdataset/', split=0.1,mode=''):\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "    \n",
    "    if mode == 'training':\n",
    "        total_size = len(images)\n",
    "        valid_size = int(split * total_size)\n",
    "        train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
    "        train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
    "\n",
    "        return (train_x, train_y), (valid_x, valid_y)\n",
    "    \n",
    "    if mode == 'testing':\n",
    "        return (images,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (256, 256))\n",
    "    x = x/255.0\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n",
    "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "    y.set_shape([256, 256, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "#     dataset = dataset.repeat()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (valid_x, valid_y) = load_data(PATH,mode='training')\n",
    "\n",
    "print(\"Training data: \", len(train_x))\n",
    "print(\"Validation data: \", len(valid_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_rgb(x):\n",
    "    x = cv2.imread(x)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_x[0]))\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_x[1]))\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_x[2]))\n",
    "imgplot.set_clim(0.0, 1.4)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 4)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_x[3]))\n",
    "imgplot.set_clim(0.0, 2.1)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_y[0]))\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_y[1]))\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_y[2]))\n",
    "imgplot.set_clim(0.0, 1.4)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 4)\n",
    "imgplot = plt.imshow(read_and_rgb(valid_y[3]))\n",
    "imgplot.set_clim(0.0, 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model from tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(inputs):\n",
    "    mobilenetv2 = tf.keras.applications.MobileNetV2(\n",
    "        input_tensor = inputs, \n",
    "        weights=\"imagenet\", include_top=False, alpha=0.35)\n",
    "    mobilenetv2.trainable = False\n",
    "    x = mobilenetv2.get_layer('out_relu').output\n",
    "    x = Conv2D(128,3,name='final_conv',padding='same',activation='relu')(x)\n",
    "    x = GlobalAveragePooling2D(name='gap')(x)\n",
    "    output = Dense(2,activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs,output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build unet from the pretrained model \n",
    "\n",
    "https://github.com/nikhilroxtomar/Unet-with-Pretrained-Encoder/blob/master/U-Net_with_Pretrained_MobileNetV2_as_Encoder.ipynb\n",
    "\n",
    "![image](https://github.com/nikhilroxtomar/Unet-with-Pretrained-Encoder/raw/5898a1e1ee66df875239d679839a30e419b20375//images/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "    \n",
    "    encoder = build_model(inputs)\n",
    "#     encoder.load_weights('../../output/checkpoints/mobilenetv2/512_128weights.16-0.31.hdf5')\n",
    "    skip_connection_names = [\"input_image\",\"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    f = [ 16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x =  Conv2DTranspose(filters=f[-i],kernel_size=3,strides=2,padding='same')(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (5, 5), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unet_plus_plus():\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluatoin Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def jackard_index(y_true, y_pred):\n",
    "    \n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) -intersection + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "def jackard_index_loss(y_true, y_pred):\n",
    "    return 1.0 - jackard_index(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf_dataset(train_x, train_y, batch=BATCH)\n",
    "valid_dataset = tf_dataset(valid_x, valid_y, batch=BATCH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "metrics = [dice_coef, jackard_index, AUC(curve='PR'), Recall(), Precision()]\n",
    "# model.load_weights('./output/unet_weights.19.0.03.hdf5')\n",
    "model.compile(loss = jackard_index_loss, optimizer=opt, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/unet/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(\n",
    "        './output/unet_weights.{epoch:02d}.{val_loss:.2f}.hdf5',\n",
    "        monitor='val_loss',save_best_only=True,save_weights_only=True,verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_steps = len(train_x)//BATCH\n",
    "valid_steps = len(valid_x)//BATCH\n",
    "\n",
    "if len(train_x) % BATCH != 0:\n",
    "    train_steps += 1\n",
    "if len(valid_x) % BATCH != 0:\n",
    "    valid_steps += 1\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=40,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./output/unet_weights.27.0.05.hdf5')\n",
    "model.compile(loss = dice_loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y= load_data(path='./dataset/LSP/test/',mode='testing')\n",
    "test_dataset = tf_dataset(test_x, test_y, batch=BATCH)\n",
    "\n",
    "print(\"test data: \", len(test_x))\n",
    "test_steps = (len(test_x)//BATCH)\n",
    "if len(test_x) % BATCH != 0:\n",
    "    test_steps += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = x/255.0\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x/255.0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_parse(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(zip(test_x[:10], test_y[:10])):\n",
    "    x = read_image(x)\n",
    "    y = read_mask(y)\n",
    "    \n",
    "    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n",
    "    \n",
    "    h, w, _ = x.shape\n",
    "    white_line = np.ones((h, 10, 3))\n",
    "\n",
    "    all_images = [\n",
    "        x, white_line,\n",
    "        mask_parse(y), white_line,\n",
    "        mask_parse(y_pred)\n",
    "    ]\n",
    "    image = np.concatenate(all_images, axis=1)\n",
    "    \n",
    "    yp = np.where(y_pred == True, 1.0, 0.0)\n",
    "    fig = plt.figure(figsize=(12, 12)) \n",
    "    a = fig.add_subplot(1, 1, 1)\n",
    "    a.axes.set_title('DC:{:.3f} JI:{:.3f}'.format(dice_coef(y,yp),jackard_index(y,yp)))\n",
    "    imgplot = plt.imshow(image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dicecoef=0\n",
    "jackardindex=0\n",
    "for  i, (x, y) in enumerate(zip(test_x, test_y)):\n",
    "    x = read_image(x)\n",
    "    y = read_mask(y)\n",
    "    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n",
    "    yp = np.where(y_pred == True, 1.0, 0.0)\n",
    "    \n",
    "\n",
    "    dicecoef += dice_coef(y,yp)\n",
    "    jackardindex += jackard_index(y,yp)\n",
    "    \n",
    "print('DC:{:.3f} JI:{:.3f}'.format(dicecoef/138,jackardindex/138))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
